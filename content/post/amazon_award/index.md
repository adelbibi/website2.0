---
title: Amazon Research Award 2022
subtitle: I was awarded the Amazon Research Award in 2022 in the Machine Learning Algorithms and Theory track.

# Summary for listings and search engines
summary: I was awarded the Amazon Research Award in 2022 in the Machine Learning Algorithms and Theory track. The project proposal is "Randomized Smoothing; Future Directions and Extensions"

# Link this post with a project
projects: []

# Date published
date: "2022-10-01T00:00:00Z"

# Date updated
lastmod: "2022-10-01T00:00:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: false


# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Amazon Science**](https://t.co/0Cw6Gx9ND6)'
  focal_point: ""
  placement: 2
  preview_only: false

authors:
- admin
- 

tags:
- 
- 

categories:
- 
- 
---

## Overview

I was awarded the Amazon Research Award in 2022 as a receipiant for the Fall 2021 track in Machine Learning Algorithms and Theory track. My project is titlted "Randomized Smoothing; Future Directions and Extensions". As part of the award, I was gifted $20,000 of AWS credits.

Read more on: https://www.amazon.science/research-awards/program-updates/fall-2021-and-winter-2022-amazon-research-awards-recipients-announced


## Summary
The startling progress over just few years is attributed to the embrace of deep neural networks (DNNs), which was possible due to the confluence of the availability of ``big data'' and powerful hardware accelerators in the form of GPUs, thus leading to the re-emergence of neural networks as the dominant paradigm. DNNs have now become the de facto approach-of-choice for a growing plethora of applications after significantly improving the state-of-the-art with many cases exceeding human level performance (e.g. AlphaGo, ALphaStar, AlphaFold, ImageNet classification, etc.). 

Despite the impressive capability they demonstrate on various benchmarks, current state-of-the-art DNNs are not robust, i.e., they are susceptible to small additive input perturbations. This is in addition to the sensitivity DNNs exhibit to non-additive semantic perturbations that occur due to slight changes in the physical world, e.g., subtle shadows, object pose variations, and varying weather conditions. Such perturbations can cause highly performing DNNs to produce incorrect predictions. We aim to advance a recent promising direction for provable robustness, namely randomized smoothing with particular interest in the theoretical advances that allow for better improved efficient certification.

