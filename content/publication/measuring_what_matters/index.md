---
title: "Measuring what Matters: Construct Validity in Large Language Model Benchmarks"

authors:
- Andrew M Bean
- Ryan Othniel Kearns
- Angelika Romanou
- Franziska Sofia Hafner
- Harry Mayne
- Jan Batzner
- Negar Foroutan
- Chris Schmitz
- Karolina Korgul
- Hunar Batra
- Oishi Deb
- Emma Beharry
- Cornelius Emde
- Thomas Foster
- Anna Gausen
- María Grandury
- Simeng Han
- Valentin Hofmann
- Lujain Ibrahim
- Hazel Kim
- Hannah Rose Kirk
- Fangru Lin
- Gabrielle Kaili-May Liu
- Lennart Luettgau
- Jabez Magomere
- Jonathan Rystrøm
- Anna Sotnikova
- Yushi Yang
- Yilun Zhao
- admin
- Antoine Bosselut
- Ronald Clark
- Arman Cohan
- Jakob Foerster
- Yarin Gal
- Scott A Hale
- Inioluwa Deborah Raji
- Christopher Summerfield
- Philip HS Torr
- Cozmin Ududec
- Luc Rocher
- Adam Mahdi

date: "2025-11-30"
doi: ""
publishDate: "2025-11-30T00:00:00Z"
publication_types: ["1"]
# Publication name and optional abbreviated publication name.
publication: Neural Information Processing Systems
publication_short: Neural Information Processing Systems (**NeurIPS25**)

abstract: "Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks."

summary: ""
tags: []
featured: true

url_pdf: 'https://arxiv.org/pdf/2511.04703'
url_code: ''
url_dataset: 'https://huggingface.co/datasets/ambean/construct-validity-review'
url_poster: ''
url_project: 'https://oxrml.com/measuring-what-matters/'
url_slides: ''
url_source: ''
url_video: ''

projects: []
slides: ''
---

<!-- Optional notes or content can go here. -->
